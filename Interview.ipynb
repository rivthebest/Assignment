{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per my understanding of the problem. to achieve result in a faster way we should not use xml or json style data. Flat text style file or CSV data would work faster than xml/json. Both in XML and JSON they are in multilevel format.\n",
    "\n",
    "Here I had used JSON data of the same source data provided as XML, because for the purpose of using it through Spark environment using through python or pyspark I haven't found any suitable example leading to that. May be I haven't exhausted my search enough. It would be better to work with XML with scala. Lots of different examples have been provided. But there is third party tool a jar file prepared by Databricks to work XML files.\n",
    "\n",
    "I had converted the xml files into json using a npm tool named xml2json. As I am using single machine so from spark point of view it is single core so for the sake of brevity I had picked up 5000 files out of 33000 files randomly.\n",
    "\n",
    "I had tried working with the values instructed in the databricks site as mentioned below:\n",
    "#import os\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS']='--packages com.databricks.spark.xml:spark-xml_2.11-0.5.0.jar pyspark-shell\n",
    "\n",
    "I had downloaded that jar and placed it into the pyspark library location but somehow it did not respond as expected or I might have done some mistakes. I would be happier if somebody helps me to understand this part.\n",
    "\n",
    "As I do have both Apache Spark and Pyspark so with the findspark utility of python I had searched for the spark environment. This is the extra tool I had used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding appropriate spark environment and initializing it.\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, SparkSession, functions as F\n",
    "from pyspark.sql.functions import explode, col, expr, isnan, udf\n",
    "from pyspark.sql.types import IntegerType, FloatType, StructType, StructField, StringType\n",
    "import pyspark\n",
    "import json\n",
    "import datetime, time\n",
    "from datetime import timedelta\n",
    "\n",
    "# Creating the Spark Session and SQL Context\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sqlContext = pyspark.SQLContext(spark)\n",
    "\n",
    "\n",
    "#reading the JSON content from the path where I put all the json data.\n",
    "path=\"/Users/rivthebest/Documents/INTERVIEW/trades-json-fraction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the RDD/Data Frame\n",
    "loadJSONDF1 = sqlContext.read.json(path, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repartitioning the Data Frame\n",
    "loanJSONDF1 = loadJSONDF1.repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan: struct (nullable = true)\n",
      " |    |-- seller: struct (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |-- trade: struct (nullable = true)\n",
      " |    |    |-- advance: struct (nullable = true)\n",
      " |    |    |    |-- amount: string (nullable = true)\n",
      " |    |    |    |-- amount_gbp: string (nullable = true)\n",
      " |    |    |    |-- date: string (nullable = true)\n",
      " |    |    |    |-- percentage: string (nullable = true)\n",
      " |    |    |-- arrears: struct (nullable = true)\n",
      " |    |    |    |-- in_arrears: string (nullable = true)\n",
      " |    |    |    |-- in_arrears_on_date: string (nullable = true)\n",
      " |    |    |-- crystalised_loss: struct (nullable = true)\n",
      " |    |    |    |-- amount: string (nullable = true)\n",
      " |    |    |    |-- date: string (nullable = true)\n",
      " |    |    |-- currency: string (nullable = true)\n",
      " |    |    |-- discount: struct (nullable = true)\n",
      " |    |    |    |-- on: string (nullable = true)\n",
      " |    |    |    |-- percentage: string (nullable = true)\n",
      " |    |    |-- expected_payment_date: string (nullable = true)\n",
      " |    |    |-- face_value: struct (nullable = true)\n",
      " |    |    |    |-- amount: string (nullable = true)\n",
      " |    |    |    |-- amount_gbp: string (nullable = true)\n",
      " |    |    |-- gross_yield: struct (nullable = true)\n",
      " |    |    |    |-- annualised: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- outstanding_principal: struct (nullable = true)\n",
      " |    |    |    |-- amount: string (nullable = true)\n",
      " |    |    |    |-- amount_gbp: string (nullable = true)\n",
      " |    |    |-- payment: struct (nullable = true)\n",
      " |    |    |    |-- state: string (nullable = true)\n",
      " |    |    |-- price_grade: string (nullable = true)\n",
      " |    |    |-- settlement_date: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing the Schema\n",
    "loanJSONDF1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(loan=Row(seller=Row(id='398'), trade=Row(advance=Row(amount='20927.0', amount_gbp='20927.0', date='2014-10-31', percentage='85.0'), arrears=Row(in_arrears='Yes', in_arrears_on_date='2014-11-26'), crystalised_loss=Row(amount='Yes', date='2014-12-08'), currency='GBP', discount=Row(on='Facevalue', percentage='0.8'), expected_payment_date='2014-12-12 00:00:00 +0000', face_value=Row(amount='24620.0', amount_gbp='24620.0'), gross_yield=Row(annualised='11.88184119551574'), id='10486', outstanding_principal=Row(amount='0.0', amount_gbp='0.0'), payment=Row(state='Repurchased'), price_grade='5', settlement_date='2014-12-08', type='MultiDebtor'))),\n",
       " Row(loan=Row(seller=Row(id='1010'), trade=Row(advance=Row(amount='19134.0', amount_gbp='13357.4454', date='2015-07-16', percentage='90.0'), arrears=Row(in_arrears='No', in_arrears_on_date='nan'), crystalised_loss=Row(amount='nan', date='nan'), currency='EUR', discount=Row(on='Advance', percentage='1.0'), expected_payment_date='2015-08-28 00:00:00 +0000', face_value=Row(amount='21260.0', amount_gbp='14841.606000000002'), gross_yield=Row(annualised='12.682503013196976'), id='16042', outstanding_principal=Row(amount='0.0', amount_gbp='0.0'), payment=Row(state='Paid'), price_grade='6', settlement_date='2015-09-09', type='Standard'))),\n",
       " Row(loan=Row(seller=Row(id='8716'), trade=Row(advance=Row(amount='108127.92', amount_gbp='108127.92', date='2015-01-13', percentage='80.0'), arrears=Row(in_arrears='No', in_arrears_on_date='nan'), crystalised_loss=Row(amount='nan', date='nan'), currency='GBP', discount=Row(on='Facevalue', percentage='0.9'), expected_payment_date='2015-04-07 00:00:00 +0000', face_value=Row(amount='135159.91', amount_gbp='135159.91'), gross_yield=Row(annualised='14.299605449973862'), id='11346', outstanding_principal=Row(amount='0.0', amount_gbp='0.0'), payment=Row(state='Paid'), price_grade='6', settlement_date='2015-04-01', type='Standard'))),\n",
       " Row(loan=Row(seller=Row(id='8946'), trade=Row(advance=Row(amount='119646.0', amount_gbp='88203.0312', date='2015-02-18', percentage='85.0'), arrears=Row(in_arrears='No', in_arrears_on_date='nan'), crystalised_loss=Row(amount='nan', date='nan'), currency='EUR', discount=Row(on='Facevalue', percentage='0.8'), expected_payment_date='2015-03-25 00:00:00 +0000', face_value=Row(amount='140760.0', amount_gbp='103768.272'), gross_yield=Row(annualised='11.88184119551574'), id='11840', outstanding_principal=Row(amount='0.0', amount_gbp='0.0'), payment=Row(state='Paid'), price_grade='5', settlement_date='2015-03-24', type='Standard'))),\n",
       " Row(loan=Row(seller=Row(id='8823'), trade=Row(advance=Row(amount='40085.59', amount_gbp='40085.59', date='2014-10-08', percentage='80.0'), arrears=Row(in_arrears='No', in_arrears_on_date='nan'), crystalised_loss=Row(amount='nan', date='nan'), currency='GBP', discount=Row(on='Facevalue', percentage='0.9'), expected_payment_date='2014-10-24 00:00:00 +0000', face_value=Row(amount='50106.95', amount_gbp='50106.95'), gross_yield=Row(annualised='14.299605449973862'), id='10228', outstanding_principal=Row(amount='0.0', amount_gbp='0.0'), payment=Row(state='Paid'), price_grade='6', settlement_date='2014-10-21', type='Standard')))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing First 5 JSON file Records\n",
    "loanJSONDF1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+---------------------+---------------+----------+\n",
      "|Seller ID|Trade ID|Trade Advance Date|Expected Payment Date|Settlement Date|Trade Type|\n",
      "+---------+--------+------------------+---------------------+---------------+----------+\n",
      "|     8398|   10422|        2014-10-30| 2014-11-30 00:00:...|     2015-01-20|  Standard|\n",
      "|    10102|   15515|        2015-06-25| 2015-07-31 00:00:...|     2015-07-10|  Standard|\n",
      "|      216|   17788|        2015-09-22| 2015-10-20 00:00:...|     2015-10-19|  Standard|\n",
      "|      173|   11026|        2014-12-10| 2015-01-09 00:00:...|     2015-01-20|  Standard|\n",
      "|     1291|   10746|        2014-11-20| 2014-12-18 00:00:...|     2014-12-01|  Standard|\n",
      "|     1837|   15132|        2015-06-10| 2015-09-11 00:00:...|     2015-09-29|  Standard|\n",
      "|    12853|   14529|        2015-05-15| 2015-07-04 00:00:...|     2016-03-21|  Standard|\n",
      "|     4727|   13046|        2015-03-04| 2015-04-17 00:00:...|     2015-05-06|  Standard|\n",
      "|     8887|   10234|        2014-10-10| 2014-11-14 00:00:...|     2014-11-17|  Standard|\n",
      "|    10053|   17629|        2015-09-15| 2015-10-30 00:00:...|     2015-11-20|  Standard|\n",
      "|    10805|   13097|        2015-03-02| 2015-04-01 00:00:...|     2015-04-09|  Standard|\n",
      "|     9123|   10920|        2014-12-03| 2014-12-31 00:00:...|     2015-01-15|  Standard|\n",
      "|     8539|   13294|        2015-03-13| 2015-05-12 00:00:...|     2015-10-06|  Standard|\n",
      "|     1018|   14596|        2015-05-20| 2015-07-01 00:00:...|     2015-07-29|  Standard|\n",
      "|     1010|   11322|        2015-01-07| 2015-01-30 00:00:...|     2015-01-16|  Standard|\n",
      "|    10879|   13150|        2015-03-04| 2015-04-10 00:00:...|     2015-04-13|  Standard|\n",
      "|      165|   10466|        2014-11-19| 2015-01-31 00:00:...|     2014-12-04|  Standard|\n",
      "|    11372|   14303|        2015-05-06| 2015-06-25 00:00:...|            nan|  Standard|\n",
      "|      165|   13964|        2015-04-21| 2015-05-29 00:00:...|     2015-06-30|  Standard|\n",
      "|     8946|   17832|        2015-09-23| 2015-11-10 00:00:...|     2015-10-22|  Standard|\n",
      "+---------+--------+------------------+---------------------+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A compact of View of Question 1\n",
    "# to show more rows we need to give numerical amount in the show function as .show(<number of rows>)\n",
    "# By default it will return 20 rows\n",
    "loanJSONDF1.select(col(\"loan.seller.id\").alias(\"Seller ID\"), \n",
    "                      col(\"loan.trade.id\").alias(\"Trade ID\"),                                         \n",
    "                      col(\"loan.trade.advance.date\").alias(\"Trade Advance Date\"),                                                              \n",
    "                      col(\"loan.trade.expected_payment_date\").alias(\"Expected Payment Date\"),                                        \n",
    "                      col(\"loan.trade.settlement_date\").alias(\"Settlement Date\"),\n",
    "                      col(\"loan.trade.type\").alias(\"Trade Type\")) \\\n",
    "              .where(col(\"loan.trade.type\").isin(\"Standard\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query with all attributes\n",
    "#For Query 1\n",
    "\n",
    "loanJSONDF1.select(col(\"loan.seller.id\").alias(\"Seller ID\"), \n",
    "                      col(\"loan.trade.id\").alias(\"Trade ID\"),\n",
    "                      col(\"loan.trade.advance.amount\").alias(\"Advance Amount\"),\n",
    "                      col(\"loan.trade.advance.amount_gbp\").alias(\"Amount in GBP\"),                     \n",
    "                      col(\"loan.trade.advance.date\").alias(\"Trade Advance Date\"),\n",
    "                      col(\"loan.trade.advance.percentage\").alias(\"Advance Percentage\"),\n",
    "                      col(\"loan.trade.arrears.in_arrears\").alias(\"Arrears\"),\n",
    "                      col(\"loan.trade.arrears.in_arrears_on_date\").alias(\"Arrears On Date\"),\n",
    "                      col(\"loan.trade.crystalised_loss.amount\").alias(\"Crystalised Loss\"),\n",
    "                      col(\"loan.trade.crystalised_loss.date\").alias(\"Crystalised Loss Date\"),\n",
    "                      col(\"loan.trade.currency\").alias(\"Currency\"),\n",
    "                      col(\"loan.trade.discount.on\").alias(\"Discount On\"),\n",
    "                      col(\"loan.trade.discount.percentage\").alias(\"Discount Percentage\"),                      \n",
    "                      col(\"loan.trade.expected_payment_date\").alias(\"Expected Payment Date\"),                                  \n",
    "                      col(\"loan.trade.face_value.amount\").alias(\"Face Value Amount\"),\n",
    "                      col(\"loan.trade.face_value.amount_gbp\").alias(\"Face Value Amount GBP\"),\n",
    "                      col(\"loan.trade.gross_yield.annualised\").alias(\"Groos Yield Annualised\"),\n",
    "                      col(\"loan.trade.outstanding_principal.amount\").alias(\"Outstanding Principal Amount\"),\n",
    "                      col(\"loan.trade.outstanding_principal.amount_gbp\").alias(\"Amount in GBP\"),\n",
    "                      col(\"loan.trade.payment.state\").alias(\"Payment State\"),\n",
    "                      col(\"loan.trade.price_grade\").alias(\"Price Grade\"),\n",
    "                      col(\"loan.trade.settlement_date\").alias(\"Settlement Date\"),\n",
    "                      col(\"loan.trade.type\").alias(\"Trade Type\")) \\\n",
    "                 .where(col(\"loan.trade.type\").isin(\"Standard\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For 2(a) Query\n",
    "\n",
    "# There is nifty python code I had found in a blog providing here with for dealing with the schema \n",
    "    # evolution everytime required for running UDF/UDAF. This python code mentioned in the below blog\n",
    "    # is a replacement of repeated botheration schema evolution\n",
    "    \n",
    "    ## http://nadbordrozd.github.io/blog/2016/05/22/one-weird-trick-that-will-fix-your-pyspark-schemas/\n",
    "\n",
    "## Creating the new Schema\n",
    "def build_schema():\n",
    "    #updating the schema with new entry duration as loan.trade.duration\n",
    "    # The schema has been placed inside a function\n",
    "       \n",
    "    newLoanSchema = StructType([\n",
    "                    StructField(\"loan\", \n",
    "                        StructType([\n",
    "                            StructField(\"seller\", \n",
    "                                StructType([\n",
    "                                        StructField('id', StringType(), True)\n",
    "                                         ]), \n",
    "                                    True),\n",
    "                            StructField(\"trade\", \n",
    "                                StructType([\n",
    "                                    StructField(\"advance\",\n",
    "                                           StructType([\n",
    "                                               StructField(\"amount\", StringType(), True),\n",
    "                                               StructField(\"amount_gbp\", StringType(), True),\n",
    "                                               StructField(\"date\", StringType(), True),\n",
    "                                               StructField(\"percentage\", StringType(), True)\n",
    "                                               ]),  True),\n",
    "                                    StructField(\"arrears\", \n",
    "                                            StructType([\n",
    "                                                StructField(\"in_arrears\", StringType(), True),\n",
    "                                                StructField(\"in_arrears_on_date\", StringType(), True)\n",
    "                                               ]), True),\n",
    "                                    StructField(\"crystalised_loss\", \n",
    "                                            StructType([\n",
    "                                                StructField(\"amount\", StringType(), True),\n",
    "                                                StructField(\"date\", StringType(), True)\n",
    "                                               ]), True),\n",
    "                                    StructField(\"currency\", StringType(), True),\n",
    "                                    StructField(\"discount\",\n",
    "                                            StructType([\n",
    "                                                StructField(\"on\", StringType(), True),\n",
    "                                                StructField(\"percentage\", StringType(), True)\n",
    "                                               ]), True),\n",
    "                                    StructField(\"expected_payment_date\", StringType(), True),\n",
    "                                    StructField(\"duration\", FloatType(), True),\n",
    "                                    StructField(\"face_value\", \n",
    "                                            StructType([\n",
    "                                                StructField(\"amount\", StringType(), True),\n",
    "                                                StructField(\"gbp\", StringType(), True)\n",
    "                                               ]), True),\n",
    "                                    StructField(\"gross_yield\", \n",
    "                                            StructType([\n",
    "                                                StructField(\"annualised\", StringType(), True)\n",
    "                                               ]), True),\n",
    "                                    StructField(\"id\", StringType(), True),\n",
    "                                    StructField(\"outstanding_principal\", \n",
    "                                            StructType([\n",
    "                                                StructField(\"amount\", StringType(), True),\n",
    "                                                StructField(\"amount_gbp\", StringType(), True)\n",
    "                                               ]), True),\n",
    "                                    StructField(\"payment\", \n",
    "                                            StructType([\n",
    "                                                StructField(\"state\", StringType(), True)\n",
    "                                               ]), True),\n",
    "                                    StructField(\"price_grade\", StringType(), True),\n",
    "                                    StructField(\"settlement_date\", StringType(), True),\n",
    "                                    StructField(\"type\", StringType(), True)\n",
    "                                          ]), True)\n",
    "                                  ]), True)\n",
    "                    ])\n",
    "    return newLoanSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'serializer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-fc0918be00de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloadJSONDF2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadJSONDF1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/workspace/APACHE/SPARK/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspace/APACHE/SPARK/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspace/APACHE/SPARK/spark-2.4.0-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, f, preservesPartitioning)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfail_on_stopiteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitionsWithIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspace/APACHE/SPARK/spark-2.4.0-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mmapPartitionsWithIndex\u001b[0;34m(self, f, preservesPartitioning)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \"\"\"\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPipelinedRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmapPartitionsWithSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspace/APACHE/SPARK/spark-2.4.0-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, prev, func, preservesPartitioning, isFromBarrier)\u001b[0m\n\u001b[1;32m   2509\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2511\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2512\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bypass_serializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreservesPartitioning\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'serializer'"
     ]
    }
   ],
   "source": [
    "#This is the difficult part where I get stuck, I am not sure where I am making mistakes\n",
    "# The error message is also provided for your reference.\n",
    "loadJSONDF2 = sqlContext.createDataFrame(loadJSONDF1.rdd, schema=build_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are all runnable codes. If the code above line is fixed. This section would be running smoothly.\n",
    "\n",
    "#Time duration calculation\n",
    "\n",
    "def time_duration(y, x):\n",
    "    y = y.split(' ')[0]\n",
    "    end = datetime.datetime.strptime(y, \"%Y-%m-%d\")\n",
    "    start = datetime.datetime.strptime(x, \"%Y-%m-%d\")\n",
    "    delta = (end - start).total_seconds()\n",
    "    delta = round((delta / (24 * 3600)))\n",
    "    \n",
    "\n",
    "# registering as udf\n",
    "ftd = udf(time_duration,newLoanSchema)\n",
    "\n",
    "# For the sake of operating the below code we might require to change the DataFrame name or may not\n",
    "\n",
    "newRDD = loanJSONDF2.withColumn(\"loan.trade.duration\",ftd(loanJSONDF2['loan.trade.expected_payment_date'],\n",
    "                                         loanJSONDF2['loan.trade.advance.date']))\n",
    "\n",
    "#printing the New Schema Again\n",
    "#loanJSONRDDNew.printSchema()\n",
    "newRDD.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query with a duration as output in a compact view\n",
    "loanJSONDF2.select(col(\"loan.seller.id\").alias(\"Seller ID\"), \n",
    "                      col(\"loan.trade.id\").alias(\"Trade ID\"),                                         \n",
    "                      col(\"loan.trade.advance.date\").alias(\"Trade Advance Date\"),                                                              \n",
    "                      col(\"loan.trade.expected_payment_date\").alias(\"Expected Payment Date\"),                   \n",
    "                      col(\"loan.trade.duration\").alias(\"Duration in Days\"),                     \n",
    "                      col(\"loan.trade.settlement_date\").alias(\"Settlement Date\"),\n",
    "                      col(\"loan.trade.type\").alias(\"Trade Type\")) \\\n",
    "              .where(col(\"loan.trade.type\").isin(\"Standard\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing date difference calculation\n",
    "import time, datetime\n",
    "from datetime import timedelta\n",
    "y=\"2011-08-31 00:00:00 +0000\"\n",
    "x=\"2011-07-31\"\n",
    "\n",
    "y = y.split(' ')[0]\n",
    "end = datetime.datetime.strptime(y, \"%Y-%m-%d\")\n",
    "start = datetime.datetime.strptime(x, \"%Y-%m-%d\")\n",
    "delta = (end - start).total_seconds() / (24 * 3600)\n",
    "delta = round(delta)\n",
    "\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+\n",
      "|   id|collect_list(loan.trade.id)|\n",
      "+-----+---------------------------+\n",
      "|13192|                    [14213]|\n",
      "|14369|       [17661, 14768, 14...|\n",
      "|15269|                    [15530]|\n",
      "|15448|       [16324, 16331, 17...|\n",
      "|15052|                    [16062]|\n",
      "|13187|                    [13616]|\n",
      "|16549|                    [17775]|\n",
      "|11251|       [13188, 12968, 18...|\n",
      "|  919|       [11393, 10987, 15...|\n",
      "|12750|       [14836, 13331, 13...|\n",
      "| 2550|       [17384, 16398, 13...|\n",
      "| 9196|       [15028, 15207, 14...|\n",
      "|17128|                    [17053]|\n",
      "|  124|                      [150]|\n",
      "|10805|       [13097, 17402, 11...|\n",
      "|10481|       [11170, 17844, 15...|\n",
      "| 8279|       [13105, 14140, 10...|\n",
      "| 8744|       [13173, 10965, 13...|\n",
      "|10668|       [13318, 11045, 13...|\n",
      "| 2495|       [16330, 14987, 10...|\n",
      "+-----+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#For 2b Query\n",
    "# I haven't work on this query for finding the aggregate details.\n",
    "loanJSONDF1.groupBy(['loan.seller.id']).agg(F.collect_list('loan.trade.id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
